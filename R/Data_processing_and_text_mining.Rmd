---
title: "**Title, abstract, and keywords: a practical guide to maximise the visibility and impact of academic papers**"
author: Patrice Pottier (he/him), Malgorzata Lagisz (she/her), Samantha Burke (she/her), Szymon M. Drobniak (he/him), Philip A. Downing (he/him), Erin L. Macartney (she/her), April Robin Martinig (she/her), Ayumi Mizuno 水野歩 (she/her), Kyle Morrison (he/him), Pietro Pollo (he/him), Lorenzo Ricolfi (he/him), Jesse Tam 譚天盈 (he/him), Coralie Williams (she/her), Yefeng Yang 杨业丰 (he/him), and Shinichi Nakagawa 中川震一 (he/him) 

date: "latest update: `r format(Sys.time(), '%d %B %Y')`"
output: 
  rmdformats::downcute:
    code_folding: show
    code_download: true
    toc_depth: 6
    toc_float:
      collapsed: false
    lightbox: true
    thumbnails: false
    downcute_theme: "chaos"
    code_overflow: wrap
editor_options: 
  chunk_output_type: console
---


<style>
#toc ul.nav li ul li {
    display: none;
    max-height: none;
}

#toc ul.nav li.active ul li  {
    display: block;
    max-height: none;
}

#toc ul.nav li ul li ul li {
    max-height: none;
    display: none !important;
}

#toc ul.nav li ul li.active ul li {
    max-height: none;
    display: block !important;
    
}

h1, h2, h3, h4, h5, h6 {
    color: #B451E1 !important;

</style>

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE, 
  tidy = TRUE,
  cache = TRUE,
  echo=TRUE
)
```

# **Load packages**
```{r}
pacman::p_load(tidyverse, 
               stringr,
               synthesisr,
               stringi,
               patchwork,
               ggdist,
               gghalves,
               ggtext,
               kableExtra)

set.seed(123) 
```

# **Process study samples**

We performed seven bibliographic searches to gather a sample of ~25 studies per journal. Below are listed how each string was processed. Details on how the search string were constructed can be found in Supplementary Information S1. 

## **Function to extract abstract, keyword and title lengths** 

This function processes bibtex files and count the number of words in the title, abstract and keywords.

```{r}
# Function to extract the data 
process_bib <- function(bib_file) {
  data <- read_refs(bib_file)
  
  # Ensure all necessary columns exist, else fill with NA
  necessary_columns <- c("title", "abstract", "keywords", "journal", "year", "volume", "number", "doi", "issn")
  missing_columns <- setdiff(necessary_columns, names(data))
  for (col in missing_columns) {
    data[[col]] <- NA
  }
  
  data <- dplyr::select(data, title, abstract, keywords, journal, year, volume, number, doi, issn)
  
  # Count number of words and characters in the title
  data$title_length <- str_count(data$title, "\\S+") # count words in the title
  data$title_character_length <- nchar(data$title) # count characters in the title
  
  # Count number of words and characters in the abstract
  data$abstract_length <- str_count(data$abstract, "\\S+") # count words in the abstract
  data$abstract_character_length <- nchar(data$abstract) # count characters in the abstract
  
  # Count number of author keywords
  data$keywords_number <- str_count(data$keywords, ";") + 1 # Count the number of semi colons (+1 because there are one fewer semi-colons than the number of keywords in each entry)

  return(data)
}

```

## **First string (all journals)** 

```{r}
# Get the list of files
dir_path <- "samples_ecoevo_journals/1st_string"
files1 <- list.files(path = dir_path, pattern = "\\.bib$", full.names = TRUE)

# Process each file
results1 <- lapply(files1, process_bib)

# Combine all results into one data frame
data1 <- do.call(rbind, results1)

data1 <- dplyr::filter(data1, abstract_length >50) # Remove very short abstracts as they are more likely to be comments/opinions
data1 <- dplyr::filter(data1, is.na(abstract_length)==FALSE) # Remove papers with missing abstracts

counts1 <- data1 %>% group_by(journal) %>% summarise(n = n())

journals_to_search1 <- dplyr::filter(counts1, n < 25)

journals_to_search1 <- distinct(left_join(journals_to_search1, select(data1, journal, issn)))

write_csv(journals_to_search1, "samples_ecoevo_journals/first_list_missing_journals.csv")
```


## **Second string (129 journals)**

```{r}
# Get the list of files
dir_path <- "samples_ecoevo_journals/2nd_string"
files2 <- list.files(path = dir_path, pattern = "\\.bib$", full.names = TRUE)

# Process each file
results2 <- lapply(files2, process_bib)

# Combine all results into one data frame
data2 <- do.call(rbind, results2)

data2 <- dplyr::filter(data2, abstract_length >50) # Remove very short abstracts as they are more likely to be comments/opinions
data2 <- dplyr::filter(data2, is.na(abstract_length)==FALSE) # Remove papers with missing abstracts

counts2 <- data2 %>% group_by(journal) %>% summarise(n = n())

journals_to_search2 <- dplyr::filter(counts2, n < 25)

journals_to_search2 <- distinct(left_join(journals_to_search2, select(data2, journal, issn))) # 61 journals left

write_csv(journals_to_search2, "samples_ecoevo_journals/second_list_missing_journals.csv")
```

## **Third string (61 journals)**

```{r}
# Get the list of files
dir_path <- "samples_ecoevo_journals/3rd_string"
files3 <- list.files(path = dir_path, pattern = "\\.bib$", full.names = TRUE)

# Process each file
results3 <- lapply(files3, process_bib)

# Combine all results into one data frame
data3 <- do.call(rbind, results3)

data3 <- dplyr::filter(data3, abstract_length >50) # Remove very short abstracts as they are more likely to be comments/opinions
data3 <- dplyr::filter(data3, is.na(abstract_length)==FALSE) # Remove papers with missing abstracts

counts3 <- data3 %>% group_by(journal) %>% summarise(n = n())

journals_to_search3 <- dplyr::filter(counts3, n < 25)

journals_to_search3 <- distinct(left_join(journals_to_search3, select(data3, journal, issn))) # 26 journals left

write_csv(journals_to_search3, "samples_ecoevo_journals/third_list_missing_journals.csv")
```


## **Fourth string (26 journals)**

```{r}
# Get the list of files
dir_path <- "samples_ecoevo_journals/4th_string"
files4 <- list.files(path = dir_path, pattern = "\\.bib$", full.names = TRUE)

# Process each file
results4 <- lapply(files4, process_bib)

# Combine all results into one data frame
data4 <- do.call(rbind, results4)

data4 <- dplyr::filter(data4, abstract_length >50) # Remove very short abstracts as they are more likely to be comments/opinions
data4 <- dplyr::filter(data4, is.na(abstract_length)==FALSE) # Remove papers with missing abstracts

counts4 <- data4 %>% group_by(journal) %>% summarise(n = n())

journals_to_search4 <- dplyr::filter(counts4, n < 25)

journals_to_search4 <- distinct(left_join(journals_to_search4, select(data4, journal, issn))) # 6 journals left

write_csv(journals_to_search4, "samples_ecoevo_journals/fourth_list_missing_journals.csv")
```


## **Fifth string (6 journals)**

```{r}
# Get the list of files
dir_path <- "samples_ecoevo_journals/5th_string"
files5 <- list.files(path = dir_path, pattern = "\\.bib$", full.names = TRUE)
# Process each file
results5 <- lapply(files5, process_bib)

# Combine all results into one data frame
data5 <- do.call(rbind, results5)

data5 <- dplyr::filter(data5, abstract_length >50) # Remove very short abstracts as they are more likely to be comments/opinions
data5 <- dplyr::filter(data5, is.na(abstract_length)==FALSE) # Remove papers with missing abstracts

counts5 <- data5 %>% group_by(journal) %>% summarise(n = n())

journals_to_search5 <- dplyr::filter(counts5, n < 25) # No journals left. 
```


## **Sixth string**

Some journals were not in the study samples because of how the search strings were constructed (i.e., when no study was published in the range of dates used for the first search string). Therefore, additional searches (6th and 7th string) were needed.

```{r}
# Get the list of files
dir_path <- "samples_ecoevo_journals/6th_string"
files6 <- list.files(path = dir_path, pattern = "\\.bib$", full.names = TRUE)

# Process each file
results6 <- lapply(files6, process_bib)

# Combine all results into one data frame
data6 <- do.call(rbind, results6)

data6 <- dplyr::filter(data6, abstract_length >50) # Remove very short abstracts as they are more likely to be comments/opinions
data6 <- dplyr::filter(data6, is.na(abstract_length)==FALSE) # Remove papers with missing abstracts

counts6 <- data6 %>% group_by(journal) %>% summarise(n = n())

journals_to_search6 <- dplyr::filter(counts6, n < 25) # No journals left. 

journals_to_search6 <- distinct(left_join(journals_to_search6, select(data6, journal, issn))) # 8 journals left

write_csv(journals_to_search6, "samples_ecoevo_journals/fifth_list_missing_journals.csv")
```

## **Seventh string** 

```{r}
# Get the list of files
dir_path <- "samples_ecoevo_journals/7th_string"
files7 <- list.files(path = dir_path, pattern = "\\.bib$", full.names = TRUE)

# Process each file
results7 <- lapply(files7, process_bib)

# Combine all results into one data frame
data7 <- do.call(rbind, results7)

data7 <- mutate(data7, journal = ifelse(is.na(journal)==TRUE, 
                                       "ANNUAL REVIEW OF ECOLOGY EVOLUTION AND SYSTEMATICS", 
                                        journal)) # Journal name for annual reviews were taken as NA

data7 <- dplyr::filter(data7, abstract_length >50) # Remove very short abstracts as they are more likely to be comments/opinions
data7 <- dplyr::filter(data7, is.na(abstract_length)==FALSE) # Remove papers with missing abstracts

counts7 <- data7 %>% group_by(journal) %>% summarise(n = n()) # 2 journals have less than 25 publications but we decided not to search further. We do not want to capture studies older than 5 years. 
```


## **Sample of studies from multidisciplinary journals** 

Study samples from multidisciplinary journals were exported manually from the Web of Science. 25 standard articles were selected for each journal so no further processing was necessary

```{r}
# Get the list of files
dir_path <- "samples_ecoevo_journals/Multidisciplinary_journals"
files_multi <- list.files(path = dir_path, pattern = "\\.bib$", full.names = TRUE)

# Process each file
results_multi <- lapply(files_multi, process_bib)

# Combine all results into one data frame
data_multi <- do.call(rbind, results_multi)
data_multi$journal <- toupper(data_multi$journal) # Upper case for journal names
```

## **Combine files** 
```{r}
data <- rbind(data1,
              data2,
              data3,
              data4,
              data5,
              data6,
              data7,
              data_multi)
```

# **Process data from journal policy survey** 

```{r}
survey <- read_csv("data/Survey_journal_guidelines.csv")
```


## **Convert character limits to word limits**  

```{r}
# Estimate relationship between word count and character count in the data
ggplot(data, aes(x = title_character_length, y = title_length)) + 
  geom_point(fill = "darkcyan", colour = "black", shape = 21, size = 3, alpha = 0.75) + 
  geom_smooth(method = "lm", linewidth = 2, colour = "black")+
  xlab("Number of characters in the title") +
  ylab ("Number of words in the title") + 
  theme_classic() + 
  theme(text = element_text(size = 20))

model <- lm(title_length ~ title_character_length, data = data)
summary(model)

# Predict this on the data from the journal survey
new_data <- survey[, c(5,6)]
new_data <- new_data %>% rename(title_length = `3.1 Title length limit`, title_character_length = `3.2 Title character limit`)

pred <- data.frame(predicted_title_length = predict(model, newdata = new_data))

pred$predicted_title_length = round(pred$predicted_title_length, 0)

survey <- cbind(survey, dplyr::select(pred, predicted_title_length))

# Now process the survey data 
survey <- survey[, c(3,5,8,11,13,16)]
survey <- survey %>% rename(journal = `2. Full journal name`,
                            title_word_limit = `3.1 Title length limit`,
                            abstract_word_limit = `5.1 Abstract length limit`, 
                            keywords_limit = `7. Keywords limit`,
                            structured_abstract = `9. Is the abstract structured?`)

survey <- survey %>% mutate(title_word_limit = ifelse(is.na(title_word_limit)==TRUE & is.na(predicted_title_length)==FALSE, 
                                                      predicted_title_length, 
                                                      title_word_limit)) %>% 
  dplyr::select(-predicted_title_length)
```


## **Standardise journal names between survey and study samples** 

Some journals were not in the study samples because of how the search strings were constructed (i.e., when no study was published in the range of dates used for the first search string). Therefore, additional searches were needed. 

```{r}
# Convert journal name to upper case
survey$journal <- str_to_upper(survey$journal)
data$journal <- str_replace_all(data$journal, "\\\\&", "&")

# Check the journal names not matching between the survey and the study sample
not_matching <- survey$journal[!survey$journal %in% data$journal]
not_matching

survey <- survey %>%
  mutate(journal = case_when(
    journal == "FIRE-BASEL" ~ "FIRE-SWITZERLAND",
    journal ==  "ECOL MANAG RESTOR" ~ "ECOLOGICAL MANAGEMENT AND RESTORATION",
    journal == "URBAN ECOSYST" ~ "URBAN ECOSYSTEMS",
    journal == "AQUAT INVASIONS" ~ "AQUATIC INVASIONS",
    journal == "INSECT SYSTEMATICS & EVOLUTION" ~ "INSECT SYSTEMATICS AND EVOLUTION",
    journal == "J NAT CONSERV" ~ "JOURNAL FOR NATURE CONSERVATION",
    journal == "WILDLIFE BIOL" ~ "WILDLIFE BIOLOGY",
    journal == "AQUAT ECOL" ~ "AQUATIC ECOLOGY",
    journal == "THEOR POPUL BIOL" ~ "THEORETICAL POPULATION BIOLOGY",
    journal == "PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES" ~ "PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA",
    journal == "MAMMAL REV" ~ "MAMMAL REVIEW",
    journal == "ECOL COMPLEX" ~ "ECOLOGICAL COMPLEXITY",
    journal == "BMC EVOLUTIONARY BIOLOGY (NAME CHANGE TO BMC ECOLOGY AND EVOLUTION)" ~ "BMC ECOLOGY AND EVOLUTION",
    journal == "SOUTHWEST NAT" ~ "SOUTHWESTERN NATURALIST",
    journal == "J WILDLIFE MANAGE" ~ "JOURNAL OF WILDLIFE MANAGEMENT",
    journal == "SOUTHEAST NAT" ~ "SOUTHEASTERN NATURALIST",
    TRUE ~ journal  # if no conditions are met, keep the original value
  ))

data <- data %>% 
  mutate(journal = case_when(
    journal == "NATURE ECOLOGY & EVOLUTION" ~ "NATURE ECOLOGY AND EVOLUTION",
    journal == "ECOLOGICAL MANAGEMENT & RESTORATION" ~ "ECOLOGICAL MANAGEMENT AND RESTORATION",
    journal == "ISRAEL JOURNAL OF ECOLOGY & EVOLUTION" ~ "ISRAEL JOURNAL OF ECOLOGY AND EVOLUTION",
    journal == "ECOHYDROLOGY & HYDROBIOLOGY" ~ "ECOHYDROLOGY AND HYDROBIOLOGY",
    journal == "COMPOST SCIENCE & UTILIZATION" ~ "COMPOST SCIENCE AND UTILIZATION", 
    journal == "INSECT SYSTEMATICS & EVOLUTION" ~ "INSECT SYSTEMATICS AND EVOLUTION",
    journal == "AFRICAN JOURNAL OF RANGE & FORAGE SCIENCE" ~ "AFRICAN JOURNAL OF RANGE AND FORAGE SCIENCE",
    journal == "RANGELAND ECOLOGY & MANAGEMENT" ~ "RANGELAND ECOLOGY AND MANAGEMENT", 
    journal == "EVOLUTION & DEVELOPMENT" ~ "EVOLUTION AND DEVELOPMENT", 
    journal == "TRENDS IN ECOLOGY & EVOLUTION"  ~ "TRENDS IN ECOLOGY AND EVOLUTION", 
    journal == "INTERNATIONAL JOURNAL OF ECOLOGY & DEVELOPMENT" ~ "INTERNATIONAL JOURNAL OF ECOLOGY AND DEVELOPMENT",
    journal == "ORGANISMS DIVERSITY & EVOLUTION"  ~ "ORGANISMS DIVERSITY AND EVOLUTION",
    journal == "AGRICULTURE ECOSYSTEMS & ENVIRONMENT" ~ "AGRICULTURE ECOSYSTEMS AND ENVIRONMENT",
    TRUE ~ journal
  ))

not_matching <- survey$journal[!survey$journal %in% data$journal]
unique(not_matching)

survey <- distinct(survey) # Some journal information was extracted twice. 
survey <- survey %>% filter(!(journal == "PALEOBIOLOGY" & is.na(abstract_word_limit))) # Remove duplicated row

# write_csv(survey, file = "data/Processed_data_survey_journal_guidelines.csv")
```


# **Combine survey results with study samples** 
```{r}
merged_data <- distinct(left_join(data, survey, by="journal"))

merged_data <- dplyr::filter(merged_data, abstract_length > 50) # Remove abstracts under 50 words as they are likely commentaries or opinion pieces

# Some rows were duplicated, where abstract_word_limit was taken as NA during merging
merged_data <- merged_data %>%
               arrange(desc(is.na(abstract_word_limit))) # Arrange so real values are at the top
merged_data <- merged_data %>%
     dplyr::filter(!duplicated(select(., -abstract_word_limit))) %>% # Identify and remove duplicates based on all columns except abstract_word_limit
     arrange(journal, title)

# There are also some duplicated rows where "journal" is NA (i.e., Annual reviews)
merged_data <- dplyr::filter(merged_data, is.na(journal) == FALSE)
```


## **Select sample of 25 studies per journal** 

A maximum of 25 studies per journal were selected to maintain comparable numbers between journals. 
```{r}
# Function to handle study selection per year
select_studies <- function(df) {
  df <- distinct(df)
  if (n_distinct(df$year) > 1) {
    return(df %>% arrange(desc(year)) %>% slice_head(n = 25)) # Select the 25 most recent ones for journals with multiple years
  } else {
    return(df %>% sample_n(size = min(25, n()))) # Randomly select 25 rows (or fewer if there are fewer than 25) for journals with only one year
  }
}

# Set seed for reproducibility
set.seed(123)

# Group by "journal" and apply the custom function
merged_data$year <- as.numeric(merged_data$year)
sample_data <- merged_data %>%
  group_by(journal) %>%
  nest() %>%
  mutate(data = map(data, select_studies)) %>%
  unnest(data) %>%
  ungroup()
```


## **Filter out non-standard article types**

### **Identify studies above and below the word limit imposed by journals**
```{r}
data_for_screening <- sample_data

# Flag studies that are 25 words above or below the word limit for abstracts.
data_for_screening <- data_for_screening %>% mutate(upper = abstract_word_limit + 25, 
                                            lower = abstract_word_limit  - 25, 
                                            longer = ifelse(abstract_length > upper, "yes", "no"),
                                            shorter = ifelse(abstract_length < lower, "yes", "no"),
                                            to_screen = ifelse(longer == "yes" | shorter == "yes", "yes", "no"))

# Avoid screening papers from journals that only publish reviews and non-standard article types or multidisciplinary journals
data_for_screening <- data_for_screening %>% mutate(to_screen2 = ifelse(journal == "ANNUAL REVIEW OF ECOLOGY EVOLUTION AND SYSTEMATICS"|
                                                                journal == "BIOLOGICAL REVIEWS" |
                                                                journal == "CURRENT OPINION IN INSECT SCIENCE"|
                                                                journal == "IDEAS IN ECOLOGY AND EVOLUTION"|
                                                                journal == "MAMMAL REVIEW"|
                                                                journal == "PERSPECTIVES IN PLANT ECOLOGY EVOLUTION AND SYSTEMATICS"|
                                                                journal == "TRENDS IN ECOLOGY AND EVOLUTION" |
                                                                journal == "NATURE" | 
                                                                journal == "NATURE COMMUNICATIONS" |
                                                                journal == "NATURE CLIMATE CHANGE" | 
                                                                journal == "SCIENTIFIC REPORTS" |
                                                                journal == "SCIENCE" |
                                                                journal == "SCIENCE ADVANCES" | 
                                                                journal == "COMMUNICATIONS BIOLOGY" |
                                                                journal == "PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA" |
                                                                journal == "PLOS BIOLOGY" | 
                                                                journal == "BIOLOGICAL REVIEWS" | 
                                                                journal == "CURRENT BIOLOGY" |
                                                                journal == "ELIFE" |
                                                                journal == "PHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY B-BIOLOGICAL SCIENCES",
                                                                "no", to_screen))
data_for_screening <- data_for_screening %>% mutate(to_screen = to_screen2) %>% select(-to_screen2)

table(data_for_screening$to_screen, useNA = "ifany")

# Subset data
studies_to_screen <- dplyr::filter(data_for_screening, to_screen =="yes") # Studies to screen
studies_not_to_screen <- dplyr::filter(data_for_screening, to_screen =="no") # Studies that we don't need to screen
studies_without_word_limits <- dplyr::filter(data_for_screening, is.na(to_screen)=="TRUE") # Studies without abstract word limits

#write_csv(studies_to_screen, file = "data/Studies_to_screen_for_article_type.csv")
```

### **Remove non-standard article types from study sample**

A total of 383 studies were excluded during the screening.
```{r}
# Import included studies during the screening
screened_articles <- read_csv("data/Included_studies_after_screening.csv") # 1938 studies included, 383 excluded
screened_articles <- select(screened_articles, title)
screened_articles <- screened_articles %>% mutate(included = "yes")

# Merge with the previous data
included_studies <- left_join(studies_to_screen, screened_articles)
included_studies <- included_studies %>% mutate(remove = ifelse(is.na(included)==TRUE, "yes", "no"))
included_studies <- dplyr::filter(included_studies, remove == "no")

# Merge with studies that were not screened
merged_data <- rbind(studies_not_to_screen, dplyr::select(included_studies, -included, - remove))
merged_data <- rbind(merged_data, studies_without_word_limits)

# Remove duplicated row 
merged_data <- merged_data  %>% 
  distinct(title, .keep_all = TRUE)

# Remove data from journals not captured in the survey
merged_data <- filter(merged_data, is.na(structured_abstract)=="FALSE") 

# Save the file with all data
all_data <- merged_data
#write_csv(all_data, "data/Processed_data_study_samples.csv")
```


# **Data summary**

## **Journal policies** 
```{r}
survey <- read_csv("data/Processed_data_survey_journal_guidelines.csv")

overall_summary_guidelines <- survey %>% 
  summarise(mean_abstract_limit = mean(abstract_word_limit, na.rm = T), 
            sd_abstract_limit=sd(abstract_word_limit, na.rm = T),
            mean_keywords_limit = mean(keywords_limit, na.rm = T),
            sd_keywords_limit = sd(keywords_limit, na.rm = T),
            mean_title_limit = mean(title_word_limit, na.rm = T),
            sd_title_limit = sd(title_word_limit, na.rm = T))

kable(overall_summary_guidelines, "html") %>% 
    row_spec(0, background = "white", color = "black", bold = TRUE) %>%
    kable_styling(fixed_thead = T, position = "left", full_width = F) %>%
    scroll_box(width = "100%", height = "150px")
    
kable(summary(survey), "html") %>% 
    row_spec(0, background = "white", color = "black", bold = TRUE) %>%
    kable_styling(fixed_thead = T, position = "left", full_width = F) %>%
    scroll_box(width = "100%", height = "400px")

# Identify the most common abstract limits 
abstract_word_freq <- table(survey$abstract_word_limit, useNA = "always")
abstract_word_freq

abstract_word_percent <- (abstract_word_freq / sum(abstract_word_freq)) * 100
abstract_word_percent

most_common_abstract_word_limit <- names(abstract_word_freq)[which.max(abstract_word_freq)]
most_common_abstract_word_limit_percent <- abstract_word_percent[most_common_abstract_word_limit]
most_common_abstract_word_limit_percent

# Identify the most common abstract limits 
keywords_freq <- table(survey$keywords_limit, useNA = "always")
keywords_freq

keywords_percent <- (keywords_freq / sum(keywords_freq)) * 100
keywords_percent

most_common_keywords_limit <- names(keywords_freq)[which.max(keywords_freq)]
most_common_keywords_limit_percent <- keywords_percent[most_common_keywords_limit]
most_common_keywords_limit_percent

# Identify the most common title limits
title_freq <- table(survey$title_word_limit, useNA = "always")
title_freq

title_percent <- (title_freq / sum(title_freq)) * 100
title_percent

most_common_title_limit <- names(title_freq)[which.max(title_freq)]
most_common_title_limit_percent <- title_percent[most_common_title_limit]
most_common_title_limit_percent

# See the proportion of journals allowing structured abstracts 
percent_structured <- survey %>% mutate(structured_abstract = ifelse(structured_abstract == "Yes", 1, 0)) %>% summarise(structured = mean(structured_abstract)*100)
percent_structured
```


## **Study samples** 

### **Abstract data**

Summary of abstract data. Here we only take data that is present in both the survey and study samples to make them comparable.

```{r}
all_data <- read_csv("data/Processed_data_study_samples.csv")

abstract_data <- dplyr::filter(all_data, is.na(abstract_word_limit) == FALSE & is.na(abstract_length) == FALSE)

summary_abstract <- 
abstract_data %>% 
  group_by(journal) %>% 
  summarise(mean_abstract_length = mean(abstract_length, na.rm = T), 
            sd_abstract_length=sd(abstract_length, na.rm = T),
            n = n())

kable(summary_abstract, "html") %>% 
    row_spec(0, background = "white", color = "black", bold = TRUE) %>%
    kable_styling(fixed_thead = T, position = "left", full_width = F) %>%
    scroll_box(width = "100%", height = "400px")

abstract_data %>% summarise(mean_abstract_length = mean(abstract_length, na.rm = T), 
                            sd_abstract_length=sd(abstract_length, na.rm = T),
                            n = n())

kable(summary(abstract_data), "html") %>% 
    row_spec(0, background = "white", color = "black", bold = TRUE) %>%
    kable_styling(fixed_thead = T, position = "left", full_width = F) %>%
    scroll_box(width = "100%", height = "400px")

### Summary in journals that don't have limits
abstract_data_no_limit <- dplyr::filter(all_data, is.na(abstract_word_limit) == TRUE)
abstract_data_no_limit %>% summarise(mean_abstract_length = mean(abstract_length, na.rm = T), 
                                     sd_abstract_length=sd(abstract_length, na.rm = T),
                                     n = n())
```

### **Keyword data**

Summary of keyword data. Here we only take data that is present in both the survey and study samples to make them comparable.

```{r}
keyword_data <- dplyr::filter(all_data, is.na(keywords_limit) == FALSE & is.na(keywords_number) == FALSE)

keyword_summary <- 
keyword_data %>% 
  group_by(journal) %>% 
  summarise(mean_keywords = mean(keywords_number, na.rm = T),
            sd_keywords = sd(keywords_number, na.rm = T),
            n = n())

kable(keyword_summary, "html") %>% 
    row_spec(0, background = "white", color = "black", bold = TRUE) %>%
    kable_styling(fixed_thead = T, position = "left", full_width = F) %>%
    scroll_box(width = "100%", height = "400px")


keyword_data %>% summarise(mean_keywords = mean(keywords_number, na.rm = T),
                           sd_keywords = sd(keywords_number, na.rm = T),
                           n = n())

kable(summary(keyword_data), "html") %>% 
    row_spec(0, background = "white", color = "black", bold = TRUE) %>%
    kable_styling(fixed_thead = T, position = "left", full_width = F) %>%
    scroll_box(width = "100%", height = "400px")

### Summary in journals that don't have limits
keyword_data_no_limit <- dplyr::filter(all_data, is.na(keywords_limit) == TRUE)
keyword_data_no_limit %>% summarise(mean_keywords = mean(keywords_number, na.rm = T),
                                    sd_keywords = sd(keywords_number, na.rm = T),
                                    n = n())
```


### **Title data**

Summary of title data. Here we only take data that is present in both the survey and study samples to make them comparable.

```{r}
title_data <- dplyr::filter(all_data, is.na(title_word_limit) == FALSE & is.na(title_length) == FALSE)

title_summary <- 
title_data %>% group_by(journal) %>% 
  summarise(mean_title = mean(title_length, na.rm = T),
                              sd_title = sd(title_length, na.rm = T),
                              n = n())

kable(title_summary, "html") %>% 
    row_spec(0, background = "white", color = "black", bold = TRUE) %>%
    kable_styling(fixed_thead = T, position = "left", full_width = F) %>%
    scroll_box(width = "100%", height = "400px")

title_data %>% summarise(mean_title = mean(title_length, na.rm = T),
                           sd_title = sd(title_length, na.rm = T),
                           n = n())

kable(summary(title_data), "html") %>% 
    row_spec(0, background = "white", color = "black", bold = TRUE) %>%
    kable_styling(fixed_thead = T, position = "left", full_width = F) %>%
    scroll_box(width = "100%", height = "400px")

### Summary in journals that don't have limits
title_data_no_limit <- dplyr::filter(all_data, is.na(title_word_limit) == TRUE)
title_data_no_limit %>% summarise(mean_title = mean(title_length, na.rm = T),
                           sd_title = sd(title_length, na.rm = T),
                           n = n())
summary(title_data_no_limit)
```

## **Count key terms repeated between sections**
```{r}
## Function to count repeated keywords
find_repeated_keywords <- function(keywords, text) {
  if (is.na(keywords) || keywords == "") return(list(number_repeated_keywords = NA, repeated_keywords = NA)) 
  
  # Convert keywords and text to lowercase
  keywords <- tolower(keywords)
  text <- tolower(text)
  
  keyword_list <- str_split(keywords, "; ") %>% unlist()  # Split the keywords at semicolons
  
  # Check each keyword according to the rules, only count if it contains at least one lowercase letter
  valid_keywords <- keyword_list[sapply(keyword_list, function(x) { grepl("[a-z]", x) })]
  
  # Check if all keywords are invalid, return NA in that case
  if(length(valid_keywords) == 0) return(list(number_repeated_keywords = NA, repeated_keywords = NA))
  
  valid_keywords <- valid_keywords[valid_keywords != ""]  # Remove any empty strings if present
  
  repeated_keywords <- c()
  
  # Find and count the number of valid keywords that are repeated in the text, ignoring case
  count <- sum(sapply(valid_keywords, function(kw) {
    kw_escaped = stringi::stri_replace_all_regex(kw, "[\\\\.+*?\\[\\^\\]$(){}=!<>|:\\-]", "\\\\\\$0")
    is_repeated <- grepl(paste0("\\b", kw_escaped, "\\b"), text)
    if (is_repeated) {
      repeated_keywords <<- c(repeated_keywords, kw)  # Append the repeated keyword to the vector
    }
    return(is_repeated)
  }))
  
  repeated_keywords_str <- paste(repeated_keywords, collapse = "; ")  # Convert to semi-colon-separated string
  
  return(list(number_repeated_keywords = count, repeated_keywords = repeated_keywords_str))
}

# Count repeated keywords
all_data$result_list <- mapply(find_repeated_keywords, all_data$keywords, all_data$abstract, SIMPLIFY = FALSE)

# Unlist the result to two separate columns
all_data <- all_data %>%
  rowwise() %>%
  mutate(
    number_repeated_keywords = list(result_list)[[1]]$number_repeated_keywords,
    repeated_keywords = list(result_list)[[1]]$repeated_keywords
  ) %>% 
  dplyr::select(-result_list) %>% 
  ungroup()

# Filtering the data to include only rows where there are keywords
filtered_data <- all_data[!is.na(all_data$keywords_number) & all_data$keywords_number > 0,]

# Calculating the proportion of studies repeating keywords
filtered_data <- mutate(filtered_data, is_repeated  = ifelse(number_repeated_keywords > 0, 1, 0))

filtered_data %>% summarise(prop = mean(is_repeated), n = n())

filtered_data %>% summarise(n_keywords = mean(number_repeated_keywords), sd = sd(number_repeated_keywords), n = n())

# Calculate the proportion of keywords repeated between sections
filtered_data <- mutate(filtered_data, proportion_repeated = number_repeated_keywords/keywords_number)

filtered_data %>% summarise(prop_repeated = mean(proportion_repeated), sd = sd(proportion_repeated), n = n())

summary(filtered_data$proportion_repeated)
```


# **Figures**

## **Figure 3A** 
```{r, fig.height= 6, fig.width=12}
# Convert word limit to a factor
abstract_data$word_limit_factor <- as.factor(abstract_data$abstract_word_limit)

annotate_df <- data.frame(
  word_limit_factor = sort(unique(abstract_data$word_limit_factor), decreasing = TRUE), # Sort in descending order
  abstract_length = c(500, 400, 350, 300, 250, 200, 150, 125, 120) # Reverse the order of the values
)

# Create plot
fig_3A <- ggplot(abstract_data, aes(x=word_limit_factor, y= abstract_length)) + 
   stat_halfeye(
    adjust = 2,
    justification = -0.5,
    .width = 0,
    point_colour = NA,
    fill= "darkcyan",
    alpha = 0.5,
    width = 0.5
  ) +
  geom_jitter(width = 0.05, alpha = 0.1, col = "darkcyan") +
  geom_boxplot(
    width = 0.4,
    outlier.color = NA,
    alpha = 0.9,
    color = "black",
    lwd = 1.15,
    #notch = TRUE,
    fill = NA
  ) +   
  geom_segment(data=annotate_df, aes(x=as.numeric(word_limit_factor)-0.075, xend=as.numeric(word_limit_factor)+0.075, y=abstract_length, yend=abstract_length), color="red", size=1.5) +
  theme_classic() +
  scale_y_continuous(limits = c(0, 700)) + 
  ylab("Abstract word count")+ 
  xlab("Abstract word limit") + 
  theme(axis.title.x = element_text (size = 24, vjust = -0.2),
        axis.title.y = element_text (size = 24, hjust = 0.5),
        axis.text.y = element_text (size = 15),
        axis.text.x = element_text (size = 15),
        panel.border = element_rect(fill=NA, size = 2)) 

fig_3A

#ggsave(file = "fig/figure_3A.png", dpi = 1000, width=12, height=6)
```

## **Figure 3B**

```{r, fig.height= 6, fig.width=12}
# Convert word limit to a factor
keyword_data$word_limit_factor <- as.factor(keyword_data$keywords_limit)

annotate_df <- data.frame(
  word_limit_factor = sort(unique(keyword_data$word_limit_factor), decreasing = TRUE), # Sort in descending order
  keywords_number = c(15, 12, 10, 8, 7, 6, 5) # Reverse the order of the values
)

fig_3B <- ggplot(keyword_data, aes(x=word_limit_factor, y= keywords_number)) + 
   stat_halfeye(
    adjust = 2,
    justification = -0.5,
    .width = 0,
    point_colour = NA,
    fill= "darkcyan",
    alpha = 0.5,
    width = 0.5
  ) +
  geom_jitter(width = 0.05, 
              height = 0.25,
              alpha = 0.1, 
              col = "darkcyan") +
  geom_boxplot(
    width = 0.4,
    outlier.color = NA,
    alpha = 0.9,
    color = "black",
    lwd=1.15,
   # notch = TRUE,
    fill = NA
  ) +   
  geom_segment(data=annotate_df, aes(x=as.numeric(word_limit_factor)-0.075, xend=as.numeric(word_limit_factor)+0.075, y=keywords_number, yend=keywords_number), color="red", size=1.5) +
  theme_classic() +
  scale_y_continuous(limits = c(0, 18)) + 
  ylab("Keyword count")+ 
  xlab("Keyword limit") + 
  theme(axis.title.x = element_text (size = 24, vjust = -0.2),
        axis.title.y = element_text (size = 24, hjust = 0.5),
        axis.text.y = element_text (size = 15),
        axis.text.x = element_text (size = 15),
        panel.border = element_rect(fill=NA, size = 2)) 

fig_3B

#ggsave(file = "fig/figure_3B.png", dpi = 1000, width=12, height=6)

```
## **Figure 3**

```{r, fig.height = 10, fig.width=12}
# Combine plots
(fig_3A/fig_3B) + plot_annotation(tag_levels = "A") & 
  theme(plot.tag = element_text(size = 25))

#ggsave(file = "fig/figure_3.png", dpi = 1000, width=12, height=10)
```

# **Software information**

```{r}
sessionInfo()
```

